#!/usr/bin/env python3
"""
AIæœåŠ¡ç›‘æ§ç³»ç»Ÿé›†æˆæµ‹è¯•

éªŒè¯PrometheusæŒ‡æ ‡æ”¶é›†ã€ç»“æ„åŒ–æ—¥å¿—å’Œæ€§èƒ½ç›‘æ§çš„é›†æˆæ•ˆæœ
"""

import sys
import os
import asyncio
import time
import json
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.services.ai_service.monitoring import (
    AIServiceMonitor, ai_monitor, monitor_ai_request,
    StructuredLogger, PrometheusMetrics
)
from app.services.ai_service.enhanced_model_selector import EnhancedModelSelector
from app.services.ai_service.openai_adapter_simple import OpenAIVisionAdapter


def print_section(title: str):
    """æ‰“å°æµ‹è¯•åŒºæ®µæ ‡é¢˜"""
    print("\n" + "="*60)
    print(f" {title}")
    print("="*60)


def test_structured_logging():
    """æµ‹è¯•ç»“æ„åŒ–æ—¥å¿—"""
    print_section("æµ‹è¯• 1: ç»“æ„åŒ–æ—¥å¿—")
    
    logger = StructuredLogger("test_monitor")
    
    print("ğŸ” æµ‹è¯•ç»“æ„åŒ–æ—¥å¿—è®°å½•...")
    
    # æ¨¡æ‹Ÿè¯·æ±‚æ—¥å¿—
    logger.log_request("gpt-4o-mini", "req_001", 
                      image_size="1024x1024", 
                      user_id="user_123")
    
    # æ¨¡æ‹Ÿå“åº”æ—¥å¿—
    logger.log_response("gpt-4o-mini", "req_001", 
                       response_time=2.5, 
                       success=True,
                       cost_usd=0.015,
                       tokens_used=350)
    
    # æ¨¡æ‹Ÿé”™è¯¯æ—¥å¿—
    logger.log_error("gpt-4-vision-preview", "req_002", 
                     error="Rate limit exceeded",
                     retry_count=3)
    
    # æ¨¡æ‹Ÿç†”æ–­å™¨æ—¥å¿—
    logger.log_circuit_breaker("ai_service", "open", 
                              failure_count=5,
                              last_failure="Connection timeout")
    
    print("âœ… ç»“æ„åŒ–æ—¥å¿—è®°å½•å®Œæˆ")
    print("ğŸ“‹ æ—¥å¿—ç‰¹æ€§:")
    print("   - JSONæ ¼å¼è¾“å‡º")
    print("   - ç»Ÿä¸€æ—¶é—´æˆ³")
    print("   - ç»“æ„åŒ–å­—æ®µ")
    print("   - å¯æœç´¢æ€§")


def test_prometheus_metrics():
    """æµ‹è¯•PrometheusæŒ‡æ ‡"""
    print_section("æµ‹è¯• 2: PrometheusæŒ‡æ ‡æ”¶é›†")
    
    metrics = PrometheusMetrics()
    
    print("ğŸ“Š æµ‹è¯•PrometheusæŒ‡æ ‡æ”¶é›†...")
    
    # è®°å½•è¯·æ±‚æŒ‡æ ‡
    for i in range(10):
        metrics.record_request("gpt-4o-mini", True)
        metrics.record_response_time("gpt-4o-mini", 2.0 + (i * 0.1))
    
    for i in range(3):
        metrics.record_request("gpt-4o-mini", False)
        metrics.record_response_time("gpt-4o-mini", 5.0)
    
    # è®°å½•å…¶ä»–æŒ‡æ ‡
    metrics.set_active_requests("gpt-4o-mini", 5)
    metrics.set_circuit_breaker_state("ai_service", "closed")
    metrics.record_cache_hit("model_cache")
    metrics.record_cache_miss("model_cache")
    metrics.update_model_info("gpt-4o-mini", "openai", "v1.0")
    
    print("âœ… PrometheusæŒ‡æ ‡è®°å½•å®Œæˆ")
    print("ğŸ“Š æŒ‡æ ‡ç±»å‹:")
    print("   - è®¡æ•°å™¨: è¯·æ±‚æ€»æ•°ã€é”™è¯¯æ•°")
    print("   - ç›´æ–¹å›¾: å“åº”æ—¶é—´åˆ†å¸ƒ")
    print("   - æµ‹é‡ä»ª: æ´»è·ƒè¯·æ±‚æ•°ã€ç†”æ–­å™¨çŠ¶æ€")
    print("   - ä¿¡æ¯: æ¨¡å‹å…ƒæ•°æ®")
    print("   - ç¼“å­˜ç»Ÿè®¡: å‘½ä¸­ç‡")
    
    if metrics.enabled:
        print("ğŸš€ PrometheusæœåŠ¡å™¨è¿è¡Œåœ¨ http://localhost:8090/metrics")
    else:
        print("âš ï¸ Prometheuså®¢æˆ·ç«¯æœªå®‰è£…ï¼Œä½¿ç”¨æ¨¡æ‹ŸæŒ‡æ ‡")


def test_performance_monitoring():
    """æµ‹è¯•æ€§èƒ½ç›‘æ§"""
    print_section("æµ‹è¯• 3: æ€§èƒ½ç›‘æ§")
    
    monitor = AIServiceMonitor(
        enable_prometheus=False,  # é¿å…ç«¯å£å†²çª
        enable_structured_logging=True,
        metrics_port=8092
    )
    
    print("ğŸ” æµ‹è¯•æ€§èƒ½ç»Ÿè®¡æ”¶é›†...")
    
    # æ¨¡æ‹Ÿä¸€ç³»åˆ—è¯·æ±‚
    models = ["gpt-4o-mini", "gpt-4-vision-preview", "claude-3-sonnet"]
    
    for model in models:
        for i in range(5):
            request_id = f"req_{model}_{i}"
            
            # è®°å½•è¯·æ±‚å¼€å§‹
            monitor.record_request_start(model, request_id)
            
            # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            processing_time = 1.0 + (i * 0.5)
            time.sleep(0.01)  # æ¨¡æ‹Ÿå¾ˆçŸ­çš„å¤„ç†
            
            # éšæœºæˆåŠŸ/å¤±è´¥
            success = i < 4  # æœ€åä¸€ä¸ªå¤±è´¥
            
            # è®°å½•è¯·æ±‚ç»“æŸ
            monitor.record_request_end(model, request_id, processing_time, success)
            
            if not success:
                monitor.record_error(model, request_id, "Simulated error")
    
    # è·å–ç»Ÿè®¡æ‘˜è¦
    summary = monitor.get_stats_summary()
    
    print("ğŸ“Š æ€§èƒ½ç»Ÿè®¡ç»“æœ:")
    for model_name, stats in summary.items():
        if model_name == "global":
            continue
        print(f"\n  {model_name}:")
        print(f"    è¯·æ±‚æ•°: {stats['request_count']}")
        print(f"    æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.2f}s")
        print(f"    æœ€è¿‘å¹³å‡å“åº”æ—¶é—´: {stats['recent_avg_response_time']:.2f}s")
    
    global_stats = summary["global"]
    print(f"\n  å…¨å±€ç»Ÿè®¡:")
    print(f"    æ€»è¯·æ±‚æ•°: {global_stats['total_requests']}")
    print(f"    æ€»é”™è¯¯æ•°: {global_stats['total_errors']}")
    print(f"    å…¨å±€æˆåŠŸç‡: {global_stats['global_success_rate']:.2%}")
    print(f"    æ´»è·ƒæ¨¡å‹æ•°: {global_stats['active_models']}")
    
    return monitor


def test_health_monitoring(monitor):
    """æµ‹è¯•å¥åº·ç›‘æ§"""
    print_section("æµ‹è¯• 4: å¥åº·ç›‘æ§")
    
    print("ğŸ¥ æµ‹è¯•å¥åº·çŠ¶æ€æ£€æŸ¥...")
    
    # è·å–å¥åº·çŠ¶æ€
    health = monitor.get_health_status()
    
    print(f"å¥åº·çŠ¶æ€: {'âœ… å¥åº·' if health['healthy'] else 'âŒ ä¸å¥åº·'}")
    
    if health['issues']:
        print(f"å‘ç°é—®é¢˜:")
        for issue in health['issues']:
            print(f"  - {issue}")
    else:
        print("æœªå‘ç°é—®é¢˜")
    
    print(f"\nå¥åº·æ£€æŸ¥æ—¶é—´: {health['timestamp']}")
    
    # åˆ›å»ºå¿«ç…§
    snapshot = monitor.create_snapshot()
    print(f"\nğŸ“¸ æ€§èƒ½å¿«ç…§:")
    print(f"  æ—¶é—´æˆ³: {snapshot.timestamp}")
    print(f"  æ€»è¯·æ±‚: {snapshot.requests_total}")
    print(f"  å¤±è´¥è¯·æ±‚: {snapshot.requests_failed}")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {snapshot.avg_response_time:.2f}s")
    print(f"  æ´»è·ƒæ¨¡å‹: {snapshot.active_models}")


@monitor_ai_request(ai_monitor)
def mock_ai_recognition(image_bytes: bytes, model_name: str = "gpt-4o-mini", request_id: str = None):
    """æ¨¡æ‹ŸAIè¯†åˆ«å‡½æ•°"""
    time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
    
    return {
        "success": True,
        "candidates": [
            {"name": "è’™å¨œä¸½è", "confidence": 0.95},
            {"name": "è¾¾èŠ¬å¥‡ä½œå“", "confidence": 0.88}
        ],
        "model_used": model_name,
        "cost_usd": 0.015
    }


@monitor_ai_request(ai_monitor)
async def mock_async_ai_recognition(image_bytes: bytes, model_name: str = "claude-3-sonnet", request_id: str = None):
    """æ¨¡æ‹Ÿå¼‚æ­¥AIè¯†åˆ«å‡½æ•°"""
    await asyncio.sleep(0.15)  # æ¨¡æ‹Ÿå¼‚æ­¥å¤„ç†æ—¶é—´
    
    return {
        "success": True,
        "candidates": [
            {"name": "æ˜Ÿå¤œ", "confidence": 0.92},
            {"name": "æ¢µé«˜ä½œå“", "confidence": 0.89}
        ],
        "model_used": model_name,
        "cost_usd": 0.025
    }


def test_decorator_integration():
    """æµ‹è¯•è£…é¥°å™¨é›†æˆ"""
    print_section("æµ‹è¯• 5: ç›‘æ§è£…é¥°å™¨é›†æˆ")
    
    print("ğŸ¨ æµ‹è¯•ç›‘æ§è£…é¥°å™¨...")
    
    # æ¸…ç†ä¹‹å‰çš„ç»Ÿè®¡
    ai_monitor.stats.clear()
    
    # æµ‹è¯•åŒæ­¥å‡½æ•°
    fake_image = b"fake_image_data" * 100
    
    print("\nğŸ“· æµ‹è¯•åŒæ­¥è¯†åˆ«:")
    result1 = mock_ai_recognition(fake_image, model_name="gpt-4o-mini", request_id="sync_req_001")
    print(f"  ç»“æœ: {result1['candidates'][0]['name']} (ç½®ä¿¡åº¦: {result1['candidates'][0]['confidence']})")
    
    result2 = mock_ai_recognition(fake_image, model_name="gpt-4o-mini", request_id="sync_req_002")
    print(f"  ç»“æœ: {result2['candidates'][0]['name']} (ç½®ä¿¡åº¦: {result2['candidates'][0]['confidence']})")
    
    print("\nğŸŒŸ æµ‹è¯•å¼‚æ­¥è¯†åˆ«:")
    
    async def test_async_calls():
        result3 = await mock_async_ai_recognition(fake_image, model_name="claude-3-sonnet", request_id="async_req_001")
        print(f"  ç»“æœ: {result3['candidates'][0]['name']} (ç½®ä¿¡åº¦: {result3['candidates'][0]['confidence']})")
        
        result4 = await mock_async_ai_recognition(fake_image, model_name="claude-3-sonnet", request_id="async_req_002")
        print(f"  ç»“æœ: {result4['candidates'][0]['name']} (ç½®ä¿¡åº¦: {result4['candidates'][0]['confidence']})")
    
    asyncio.run(test_async_calls())
    
    # æ˜¾ç¤ºç›‘æ§ç»Ÿè®¡
    summary = ai_monitor.get_stats_summary()
    print(f"\nğŸ“Š è£…é¥°å™¨ç›‘æ§ç»Ÿè®¡:")
    
    for model_name, stats in summary.items():
        if model_name == "global":
            continue
        print(f"  {model_name}:")
        print(f"    è¯·æ±‚æ•°: {stats['request_count']}")
        print(f"    æˆåŠŸç‡: {stats['success_rate']:.2%}")
        print(f"    å¹³å‡å“åº”æ—¶é—´: {stats['avg_response_time']:.3f}s")


def test_integration_with_enhanced_selector():
    """æµ‹è¯•ä¸å¢å¼ºé€‰æ‹©å™¨çš„é›†æˆ"""
    print_section("æµ‹è¯• 6: ä¸å¢å¼ºé€‰æ‹©å™¨é›†æˆ")
    
    print("ğŸ¤– æµ‹è¯•ç›‘æ§ä¸å¢å¼ºé€‰æ‹©å™¨é›†æˆ...")
    
    # åˆ›å»ºé€‚é…å™¨
    adapters = [
        OpenAIVisionAdapter(
            api_key="sk-test-key-12345678901234567890",
            model_name="gpt-4o-mini"
        ),
        OpenAIVisionAdapter(
            api_key="sk-test-key-12345678901234567890",
            model_name="gpt-4-vision-preview"
        )
    ]
    
    # åˆ›å»ºå¢å¼ºé€‰æ‹©å™¨
    enhanced_selector = EnhancedModelSelector(adapters=adapters)
    
    # æ¨¡æ‹Ÿç›‘æ§é›†æˆ
    test_monitor = AIServiceMonitor(
        enable_prometheus=False,
        enable_structured_logging=True
    )
    
    # æ¨¡æ‹Ÿé€‰æ‹©å™¨æ“ä½œç›‘æ§
    async def simulate_selector_operations():
        for strategy in ["cost", "accuracy", "speed", "balanced"]:
            request_id = f"selector_req_{strategy}"
            
            try:
                test_monitor.record_request_start("model_selector", request_id)
                
                start_time = time.time()
                adapter = await enhanced_selector.select_best_model(strategy=strategy)
                duration = time.time() - start_time
                
                test_monitor.record_request_end("model_selector", request_id, duration, True)
                
                print(f"  âœ… {strategy.upper()} ç­–ç•¥: {adapter.model_name} ({duration:.3f}s)")
                
            except Exception as e:
                duration = time.time() - start_time
                test_monitor.record_request_end("model_selector", request_id, duration, False)
                test_monitor.record_error("model_selector", request_id, str(e))
                
                print(f"  âŒ {strategy.upper()} ç­–ç•¥å¤±è´¥: {e}")
    
    asyncio.run(simulate_selector_operations())
    
    # æ˜¾ç¤ºç›‘æ§ç»“æœ
    summary = test_monitor.get_stats_summary()
    selector_stats = summary.get("model_selector", {})
    
    if selector_stats:
        print(f"\nğŸ“Š é€‰æ‹©å™¨ç›‘æ§ç»Ÿè®¡:")
        print(f"  è¯·æ±‚æ•°: {selector_stats['request_count']}")
        print(f"  æˆåŠŸç‡: {selector_stats['success_rate']:.2%}")
        print(f"  å¹³å‡å“åº”æ—¶é—´: {selector_stats['avg_response_time']:.3f}s")


def test_monitoring_dashboard_data():
    """æµ‹è¯•ç›‘æ§ä»ªè¡¨æ¿æ•°æ®"""
    print_section("æµ‹è¯• 7: ç›‘æ§ä»ªè¡¨æ¿æ•°æ®")
    
    print("ğŸ“ˆ ç”Ÿæˆç›‘æ§ä»ªè¡¨æ¿æ•°æ®...")
    
    # ä½¿ç”¨å…¨å±€ç›‘æ§å™¨çš„æ•°æ®
    summary = ai_monitor.get_stats_summary()
    health = ai_monitor.get_health_status()
    
    # ç”Ÿæˆä»ªè¡¨æ¿æ•°æ®
    dashboard_data = {
        "timestamp": time.time(),
        "health": health,
        "summary": summary,
        "metrics": {
            "total_requests": summary["global"]["total_requests"],
            "error_rate": summary["global"]["total_errors"] / max(1, summary["global"]["total_requests"]),
            "avg_response_time": summary["global"]["global_avg_response_time"],
            "active_models": summary["global"]["active_models"]
        },
        "models": {}
    }
    
    # æ·»åŠ æ¯ä¸ªæ¨¡å‹çš„è¯¦ç»†æ•°æ®
    for model_name, stats in summary.items():
        if model_name != "global":
            dashboard_data["models"][model_name] = {
                "requests": stats["request_count"],
                "success_rate": stats["success_rate"],
                "avg_response_time": stats["avg_response_time"],
                "last_request": stats["last_request_time"]
            }
    
    print("âœ… ä»ªè¡¨æ¿æ•°æ®ç”Ÿæˆå®Œæˆ")
    print(f"ğŸ“Š æ•°æ®æ‘˜è¦:")
    print(f"  æ€»è¯·æ±‚æ•°: {dashboard_data['metrics']['total_requests']}")
    print(f"  é”™è¯¯ç‡: {dashboard_data['metrics']['error_rate']:.2%}")
    print(f"  å¹³å‡å“åº”æ—¶é—´: {dashboard_data['metrics']['avg_response_time']:.3f}s")
    print(f"  æ´»è·ƒæ¨¡å‹æ•°: {dashboard_data['metrics']['active_models']}")
    print(f"  å¥åº·çŠ¶æ€: {'âœ… å¥åº·' if dashboard_data['health']['healthy'] else 'âŒ ä¸å¥åº·'}")
    
    return dashboard_data


async def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª AIæœåŠ¡ç›‘æ§ç³»ç»Ÿé›†æˆæµ‹è¯•å¼€å§‹")
    print("=" * 60)
    print("æœ¬æµ‹è¯•éªŒè¯ä¼ä¸šçº§ç›‘æ§åŠŸèƒ½:")
    print("âœ… ç»“æ„åŒ–æ—¥å¿—è®°å½•")
    print("âœ… PrometheusæŒ‡æ ‡æ”¶é›†")
    print("âœ… æ€§èƒ½ç»Ÿè®¡ç›‘æ§")
    print("âœ… å¥åº·çŠ¶æ€æ£€æŸ¥")
    print("âœ… ç›‘æ§è£…é¥°å™¨")
    print("âœ… ä¸AIç»„ä»¶é›†æˆ")
    print("âœ… ä»ªè¡¨æ¿æ•°æ®ç”Ÿæˆ")
    
    try:
        # 1. ç»“æ„åŒ–æ—¥å¿—æµ‹è¯•
        test_structured_logging()
        
        # 2. PrometheusæŒ‡æ ‡æµ‹è¯•
        test_prometheus_metrics()
        
        # 3. æ€§èƒ½ç›‘æ§æµ‹è¯•
        monitor = test_performance_monitoring()
        
        # 4. å¥åº·ç›‘æ§æµ‹è¯•
        test_health_monitoring(monitor)
        
        # 5. è£…é¥°å™¨é›†æˆæµ‹è¯•
        test_decorator_integration()
        
        # 6. ä¸å¢å¼ºé€‰æ‹©å™¨é›†æˆæµ‹è¯•
        await test_integration_with_enhanced_selector()
        
        # 7. ä»ªè¡¨æ¿æ•°æ®æµ‹è¯•
        dashboard_data = test_monitoring_dashboard_data()
        
        print_section("æµ‹è¯•å®Œæˆ")
        print("âœ… æ‰€æœ‰ç›‘æ§åŠŸèƒ½æµ‹è¯•å®Œæˆï¼")
        print("\nğŸ¯ ä¼ä¸šçº§ç›‘æ§ç³»ç»Ÿç‰¹æ€§éªŒè¯:")
        print("   âœ… ç»“æ„åŒ–JSONæ—¥å¿— - ä¾¿äºæœç´¢å’Œåˆ†æ")
        print("   âœ… PrometheusæŒ‡æ ‡ - ä¸ç›‘æ§ç”Ÿæ€é›†æˆ")
        print("   âœ… å®æ—¶æ€§èƒ½ç»Ÿè®¡ - å“åº”æ—¶é—´ã€æˆåŠŸç‡è¿½è¸ª")
        print("   âœ… å¥åº·çŠ¶æ€ç›‘æ§ - è‡ªåŠ¨é—®é¢˜æ£€æµ‹")
        print("   âœ… ç›‘æ§è£…é¥°å™¨ - æ— ä¾µå…¥å¼é›†æˆ")
        print("   âœ… ç»„ä»¶é›†æˆ - ä¸AIæœåŠ¡æ— ç¼åä½œ")
        print("   âœ… ä»ªè¡¨æ¿æ”¯æŒ - å¯è§†åŒ–ç›‘æ§æ•°æ®")
        print("\nğŸš€ ç›‘æ§ç³»ç»Ÿå·²å‡†å¤‡å°±ç»ªç”¨äºç”Ÿäº§ç¯å¢ƒï¼")
        print("ğŸ“Š ç›‘æ§ç«¯ç‚¹:")
        print("   - PrometheusæŒ‡æ ‡: http://localhost:8090/metrics")
        print("   - åº”ç”¨å¥åº·æ£€æŸ¥: é€šè¿‡AIServiceMonitor.get_health_status()")
        print("   - æ€§èƒ½ç»Ÿè®¡: é€šè¿‡AIServiceMonitor.get_stats_summary()")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨AIæœåŠ¡ç›‘æ§ç³»ç»Ÿé›†æˆæµ‹è¯•...")
    print("è¯·ç¡®ä¿ä½ åœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹è¿è¡Œæ­¤è„šæœ¬")
    print()
    
    # è¿è¡Œå¼‚æ­¥æµ‹è¯•
    asyncio.run(main())