"""
API Performance Benchmark Tests
æµ‹è¯•APIç«¯ç‚¹çš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å“åº”æ—¶é—´ã€ååé‡å’Œå¹¶å‘å¤„ç†èƒ½åŠ›
"""

import asyncio
import time
import statistics
from typing import List, Dict, Any
import aiohttp
import hashlib
from dataclasses import dataclass
from datetime import datetime
import json
import random
import os

# Performance targets from architecture requirements
PERFORMANCE_TARGETS = {
    "l1_cache_response_ms": 10,      # L1ç¼“å­˜å“åº”æ—¶é—´ < 10ms
    "l2_cache_response_ms": 100,     # L2ç¼“å­˜å“åº”æ—¶é—´ < 100ms 
    "api_response_ms": 200,          # APIå“åº”æ—¶é—´ < 200ms (cache hit)
    "cache_hit_rate": 70,            # ç¼“å­˜å‘½ä¸­ç‡ > 70%
    "popular_hit_rate": 90,          # çƒ­é—¨å†…å®¹å‘½ä¸­ç‡ > 90%
    "concurrent_users": 100,         # æ”¯æŒ100å¹¶å‘ç”¨æˆ·
    "throughput_rps": 500,           # ååé‡ > 500 req/s
}

@dataclass
class BenchmarkResult:
    """æ€§èƒ½æµ‹è¯•ç»“æœ"""
    endpoint: str
    total_requests: int
    successful_requests: int
    failed_requests: int
    min_response_time: float
    max_response_time: float
    avg_response_time: float
    p50_response_time: float
    p95_response_time: float
    p99_response_time: float
    throughput_rps: float
    cache_hit_rate: float
    error_rate: float
    timestamp: datetime

class APIBenchmark:
    """APIæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    def __init__(self, base_url: str = "http://localhost:8000"):
        self.base_url = base_url
        self.session = None
        self.results = []
        
    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()
    
    async def benchmark_recognition_endpoint(
        self, 
        num_requests: int = 100,
        concurrent: int = 10,
        use_same_image: bool = False
    ) -> BenchmarkResult:
        """æµ‹è¯•å›¾åƒè¯†åˆ«ç«¯ç‚¹æ€§èƒ½"""
        
        print(f"\nğŸ“Š Testing Recognition Endpoint")
        print(f"   Requests: {num_requests}, Concurrent: {concurrent}")
        
        # å‡†å¤‡æµ‹è¯•å›¾åƒ
        test_images = []
        if use_same_image:
            # ä½¿ç”¨ç›¸åŒå›¾åƒæµ‹è¯•ç¼“å­˜æ€§èƒ½
            image_data = self._generate_test_image()
            test_images = [image_data] * num_requests
        else:
            # ä½¿ç”¨ä¸åŒå›¾åƒæµ‹è¯•çœŸå®åœºæ™¯
            test_images = [self._generate_test_image(i) for i in range(num_requests)]
        
        # æ‰§è¡ŒåŸºå‡†æµ‹è¯•
        start_time = time.time()
        tasks = []
        
        for i in range(num_requests):
            task = self._test_recognition(test_images[i % len(test_images)])
            tasks.append(task)
            
            # æ§åˆ¶å¹¶å‘
            if len(tasks) >= concurrent:
                results = await asyncio.gather(*tasks, return_exceptions=True)
                tasks = []
        
        # å¤„ç†å‰©ä½™ä»»åŠ¡
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
        
        total_time = time.time() - start_time
        
        # åˆ†æç»“æœ
        response_times = []
        cache_hits = 0
        errors = 0
        
        for result in results:
            if isinstance(result, Exception):
                errors += 1
            else:
                response_times.append(result['response_time'])
                if result.get('cached', False):
                    cache_hits += 1
        
        # è®¡ç®—ç»Ÿè®¡æ•°æ®
        if response_times:
            response_times.sort()
            
            result = BenchmarkResult(
                endpoint="/api/v1/recognize",
                total_requests=num_requests,
                successful_requests=len(response_times),
                failed_requests=errors,
                min_response_time=min(response_times) * 1000,  # Convert to ms
                max_response_time=max(response_times) * 1000,
                avg_response_time=statistics.mean(response_times) * 1000,
                p50_response_time=response_times[len(response_times)//2] * 1000,
                p95_response_time=response_times[int(len(response_times)*0.95)] * 1000,
                p99_response_time=response_times[int(len(response_times)*0.99)] * 1000,
                throughput_rps=num_requests / total_time,
                cache_hit_rate=(cache_hits / len(response_times)) * 100,
                error_rate=(errors / num_requests) * 100,
                timestamp=datetime.now()
            )
            
            self.results.append(result)
            return result
        else:
            raise Exception("All requests failed")
    
    async def _test_recognition(self, image_data: bytes) -> Dict[str, Any]:
        """æµ‹è¯•å•ä¸ªè¯†åˆ«è¯·æ±‚"""
        url = f"{self.base_url}/api/v1/recognize"
        
        start_time = time.time()
        
        try:
            data = aiohttp.FormData()
            data.add_field('file', image_data, filename='test.jpg', content_type='image/jpeg')
            
            async with self.session.post(url, data=data) as response:
                response_time = time.time() - start_time
                
                if response.status == 200:
                    result = await response.json()
                    return {
                        'success': True,
                        'response_time': response_time,
                        'cached': result.get('cached', False),
                        'confidence': result.get('confidence', 0)
                    }
                else:
                    return {
                        'success': False,
                        'response_time': response_time,
                        'error': f"HTTP {response.status}"
                    }
                    
        except Exception as e:
            return {
                'success': False,
                'response_time': time.time() - start_time,
                'error': str(e)
            }
    
    async def benchmark_cache_performance(self) -> Dict[str, Any]:
        """æµ‹è¯•ç¼“å­˜æ€§èƒ½"""
        
        print(f"\nğŸ“Š Testing Cache Performance")
        
        # é¢„çƒ­ç¼“å­˜
        print("   Warming up cache...")
        warm_up_images = [self._generate_test_image(i) for i in range(10)]
        for img in warm_up_images:
            await self._test_recognition(img)
        
        # æµ‹è¯•ç¼“å­˜å‘½ä¸­ç‡
        print("   Testing cache hit rate...")
        
        # ç¬¬ä¸€è½®ï¼šæ–°å›¾åƒï¼ˆç¼“å­˜æœªå‘½ä¸­ï¼‰
        cold_result = await self.benchmark_recognition_endpoint(
            num_requests=50,
            concurrent=5,
            use_same_image=False
        )
        
        # ç¬¬äºŒè½®ï¼šç›¸åŒå›¾åƒï¼ˆç¼“å­˜å‘½ä¸­ï¼‰
        warm_result = await self.benchmark_recognition_endpoint(
            num_requests=50,
            concurrent=5,
            use_same_image=True
        )
        
        # ç¬¬ä¸‰è½®ï¼šæ··åˆåœºæ™¯
        mixed_images = []
        for i in range(100):
            if i % 3 == 0:  # 33% new images
                mixed_images.append(self._generate_test_image(1000 + i))
            else:  # 67% repeated images
                mixed_images.append(warm_up_images[i % len(warm_up_images)])
        
        mixed_result = await self.benchmark_recognition_endpoint(
            num_requests=100,
            concurrent=10,
            use_same_image=False
        )
        
        return {
            "cold_cache": self._format_result(cold_result),
            "warm_cache": self._format_result(warm_result),
            "mixed_scenario": self._format_result(mixed_result),
            "performance_improvement": {
                "response_time_reduction": f"{((cold_result.avg_response_time - warm_result.avg_response_time) / cold_result.avg_response_time * 100):.1f}%",
                "cache_effectiveness": warm_result.cache_hit_rate
            }
        }
    
    async def benchmark_concurrent_load(self) -> Dict[str, Any]:
        """æµ‹è¯•å¹¶å‘è´Ÿè½½"""
        
        print(f"\nğŸ“Š Testing Concurrent Load")
        
        concurrent_levels = [1, 5, 10, 20, 50, 100]
        results = {}
        
        for concurrent in concurrent_levels:
            print(f"   Testing {concurrent} concurrent users...")
            
            result = await self.benchmark_recognition_endpoint(
                num_requests=100,
                concurrent=concurrent,
                use_same_image=False
            )
            
            results[f"{concurrent}_users"] = {
                "avg_response_time_ms": result.avg_response_time,
                "p95_response_time_ms": result.p95_response_time,
                "throughput_rps": result.throughput_rps,
                "error_rate": result.error_rate
            }
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æ€§èƒ½ç›®æ ‡
            if result.avg_response_time > PERFORMANCE_TARGETS["api_response_ms"]:
                print(f"   âš ï¸  Response time {result.avg_response_time:.1f}ms exceeds target {PERFORMANCE_TARGETS['api_response_ms']}ms")
        
        return results
    
    async def benchmark_database_queries(self) -> Dict[str, Any]:
        """æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½"""
        
        print(f"\nğŸ“Š Testing Database Query Performance")
        
        endpoints = [
            ("/api/v1/artworks/popular", "Popular Artworks"),
            ("/api/v1/museums", "Museums List"),
            ("/api/v1/user/profile", "User Profile"),
        ]
        
        results = {}
        
        for endpoint, name in endpoints:
            print(f"   Testing {name}...")
            
            response_times = []
            for _ in range(50):
                start_time = time.time()
                
                try:
                    async with self.session.get(f"{self.base_url}{endpoint}") as response:
                        if response.status == 200:
                            await response.json()
                        response_times.append(time.time() - start_time)
                except Exception as e:
                    print(f"   Error: {e}")
            
            if response_times:
                results[name] = {
                    "min_ms": min(response_times) * 1000,
                    "avg_ms": statistics.mean(response_times) * 1000,
                    "max_ms": max(response_times) * 1000,
                    "p95_ms": sorted(response_times)[int(len(response_times)*0.95)] * 1000
                }
        
        return results
    
    def _generate_test_image(self, seed: int = None) -> bytes:
        """ç”Ÿæˆæµ‹è¯•å›¾åƒæ•°æ®"""
        # ç®€å•çš„æµ‹è¯•å›¾åƒç”Ÿæˆï¼ˆå®é™…åº”ä½¿ç”¨çœŸå®å›¾åƒï¼‰
        if seed is not None:
            random.seed(seed)
        
        # ç”Ÿæˆéšæœºå­—èŠ‚ä½œä¸ºæ¨¡æ‹Ÿå›¾åƒ
        size = random.randint(100_000, 500_000)  # 100KB - 500KB
        return bytes(random.randint(0, 255) for _ in range(size))
    
    def _format_result(self, result: BenchmarkResult) -> Dict[str, Any]:
        """æ ¼å¼åŒ–æµ‹è¯•ç»“æœ"""
        return {
            "response_times": {
                "min_ms": result.min_response_time,
                "avg_ms": result.avg_response_time,
                "p50_ms": result.p50_response_time,
                "p95_ms": result.p95_response_time,
                "p99_ms": result.p99_response_time,
                "max_ms": result.max_response_time,
            },
            "throughput_rps": result.throughput_rps,
            "cache_hit_rate": result.cache_hit_rate,
            "error_rate": result.error_rate,
            "success_rate": (result.successful_requests / result.total_requests) * 100
        }
    
    def generate_report(self) -> str:
        """ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š"""
        
        report = []
        report.append("=" * 80)
        report.append("API PERFORMANCE BENCHMARK REPORT")
        report.append("=" * 80)
        report.append(f"Timestamp: {datetime.now().isoformat()}")
        report.append(f"Base URL: {self.base_url}")
        report.append("")
        
        # Performance Targets
        report.append("PERFORMANCE TARGETS:")
        report.append("-" * 40)
        for key, value in PERFORMANCE_TARGETS.items():
            report.append(f"  {key}: {value}")
        report.append("")
        
        # Test Results
        report.append("TEST RESULTS:")
        report.append("-" * 40)
        
        for result in self.results:
            report.append(f"\nEndpoint: {result.endpoint}")
            report.append(f"  Total Requests: {result.total_requests}")
            report.append(f"  Successful: {result.successful_requests}")
            report.append(f"  Failed: {result.failed_requests}")
            report.append(f"  Error Rate: {result.error_rate:.1f}%")
            report.append(f"  Cache Hit Rate: {result.cache_hit_rate:.1f}%")
            report.append(f"  Throughput: {result.throughput_rps:.1f} req/s")
            report.append(f"  Response Times (ms):")
            report.append(f"    Min: {result.min_response_time:.1f}")
            report.append(f"    Avg: {result.avg_response_time:.1f}")
            report.append(f"    P50: {result.p50_response_time:.1f}")
            report.append(f"    P95: {result.p95_response_time:.1f}")
            report.append(f"    P99: {result.p99_response_time:.1f}")
            report.append(f"    Max: {result.max_response_time:.1f}")
            
            # Check against targets
            report.append(f"  Target Compliance:")
            
            if result.avg_response_time <= PERFORMANCE_TARGETS["api_response_ms"]:
                report.append(f"    âœ… Response Time: {result.avg_response_time:.1f}ms <= {PERFORMANCE_TARGETS['api_response_ms']}ms")
            else:
                report.append(f"    âŒ Response Time: {result.avg_response_time:.1f}ms > {PERFORMANCE_TARGETS['api_response_ms']}ms")
            
            if result.cache_hit_rate >= PERFORMANCE_TARGETS["cache_hit_rate"]:
                report.append(f"    âœ… Cache Hit Rate: {result.cache_hit_rate:.1f}% >= {PERFORMANCE_TARGETS['cache_hit_rate']}%")
            else:
                report.append(f"    âŒ Cache Hit Rate: {result.cache_hit_rate:.1f}% < {PERFORMANCE_TARGETS['cache_hit_rate']}%")
            
            if result.throughput_rps >= PERFORMANCE_TARGETS["throughput_rps"]:
                report.append(f"    âœ… Throughput: {result.throughput_rps:.1f} req/s >= {PERFORMANCE_TARGETS['throughput_rps']} req/s")
            else:
                report.append(f"    âŒ Throughput: {result.throughput_rps:.1f} req/s < {PERFORMANCE_TARGETS['throughput_rps']} req/s")
        
        report.append("")
        report.append("=" * 80)
        
        return "\n".join(report)

async def main():
    """è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•"""
    
    async with APIBenchmark() as benchmark:
        
        # 1. æµ‹è¯•ç¼“å­˜æ€§èƒ½
        cache_results = await benchmark.benchmark_cache_performance()
        print("\nğŸ“ˆ Cache Performance Results:")
        print(json.dumps(cache_results, indent=2))
        
        # 2. æµ‹è¯•å¹¶å‘è´Ÿè½½
        concurrent_results = await benchmark.benchmark_concurrent_load()
        print("\nğŸ“ˆ Concurrent Load Results:")
        print(json.dumps(concurrent_results, indent=2))
        
        # 3. æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢
        db_results = await benchmark.benchmark_database_queries()
        print("\nğŸ“ˆ Database Query Results:")
        print(json.dumps(db_results, indent=2))
        
        # 4. ç”ŸæˆæŠ¥å‘Š
        report = benchmark.generate_report()
        print("\n" + report)
        
        # ä¿å­˜æŠ¥å‘Š
        report_file = f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w') as f:
            f.write(report)
        print(f"\nâœ… Report saved to {report_file}")

if __name__ == "__main__":
    asyncio.run(main())